{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923c259a-a696-4511-bfcc-b88cfa7c58b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:28.770083Z",
     "iopub.status.busy": "2025-04-13T17:18:28.769792Z",
     "iopub.status.idle": "2025-04-13T17:18:28.774295Z",
     "shell.execute_reply": "2025-04-13T17:18:28.773440Z",
     "shell.execute_reply.started": "2025-04-13T17:18:28.770062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install transformers (if not already installed)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6d417-6161-440c-be0e-e0a8f4aee3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:29.329698Z",
     "iopub.status.busy": "2025-04-13T17:18:29.329376Z",
     "iopub.status.idle": "2025-04-13T17:18:29.334525Z",
     "shell.execute_reply": "2025-04-13T17:18:29.333626Z",
     "shell.execute_reply.started": "2025-04-13T17:18:29.329671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The classifier is built on top of the famous BERT model, which is great at understanding text. \n",
    "We will then add a dropout layer to keep things in check and a linear layer to help us classify text.\n",
    "Our BERTClassifier takes in some input IDs and attention masks, and runs them through BERT and the extra layers we added. \n",
    "The classifier returns our output as class scores.\n",
    "'''\n",
    "class CaptionClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels=2):\n",
    "        super(CaptionClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f7867-96b7-4ef2-81f3-6256bb57ddfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:30.376860Z",
     "iopub.status.busy": "2025-04-13T17:18:30.376567Z",
     "iopub.status.idle": "2025-04-13T17:18:30.382230Z",
     "shell.execute_reply": "2025-04-13T17:18:30.381442Z",
     "shell.execute_reply.started": "2025-04-13T17:18:30.376839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Dataset for the caption classifier.\n",
    "    Each sample is a tuple: (original_caption, generated_caption, occlusion_level, label)\n",
    "    the input text is formed as mentioned in assignment \n",
    "\"\"\"\n",
    "class CaptionClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        orig_cap, gen_cap, occlusion_level, label = self.data[idx]\n",
    "        input_text = f\"{orig_cap} {tokenizer.sep_token} {gen_cap} {tokenizer.sep_token} {occlusion_level}\"\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()  # shape: [max_length]\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dd700-0b13-4b8e-8edf-686ca0112335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:30.564317Z",
     "iopub.status.busy": "2025-04-13T17:18:30.563973Z",
     "iopub.status.idle": "2025-04-13T17:18:30.971685Z",
     "shell.execute_reply": "2025-04-13T17:18:30.971003Z",
     "shell.execute_reply.started": "2025-04-13T17:18:30.564290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5196 rows, Val: 742 rows, Test: 1486 rows\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code combines two files into one csv file \n",
    "and rearrange them so as to make train, validation and test sets \n",
    "'''\n",
    "\n",
    "df_smol = pd.read_csv(\"/kaggle/input/partc-bert/occlusion_details_SmolVLM.csv\")\n",
    "df_custom = pd.read_csv(\"/kaggle/input/partc-bert/occlusion_details_custom.csv\")\n",
    "\n",
    "# Add label: 0 for SmolVLM, 1 for Custom model\n",
    "df_smol[\"label\"] = 0\n",
    "df_custom[\"label\"] = 1\n",
    "df_smol[\"image_id\"] = df_smol.index\n",
    "df_custom[\"image_id\"] = df_custom.index\n",
    "df_combined = pd.concat([df_smol, df_custom], ignore_index=True)\n",
    "unique_ids = df_smol[\"image_id\"].unique()\n",
    "np.random.shuffle(unique_ids)\n",
    "\n",
    "n = len(unique_ids)\n",
    "train_ids = unique_ids[:int(0.7 * n)]\n",
    "val_ids = unique_ids[int(0.7 * n):int(0.8 * n)]\n",
    "test_ids = unique_ids[int(0.8 * n):]\n",
    "\n",
    "def get_split(df, ids):\n",
    "    # Get all rows whose image_id is in ids.\n",
    "    return df[df[\"image_id\"].isin(ids)]\n",
    "\n",
    "df_train = get_split(df_combined, train_ids)\n",
    "df_val = get_split(df_combined, val_ids)\n",
    "df_test = get_split(df_combined, test_ids)\n",
    "\n",
    "print(f\"Train: {len(df_train)} rows, Val: {len(df_val)} rows, Test: {len(df_test)} rows\")\n",
    "def create_data_list(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Ensure occlusion_level is represented as int or str if needed.\n",
    "        data_list.append((row[\"original_caption\"], row[\"generated_caption\"], row[\"occlusion_level\"], row[\"label\"]))\n",
    "    return data_list\n",
    "\n",
    "train_data = create_data_list(df_train)\n",
    "val_data = create_data_list(df_val)\n",
    "test_data = create_data_list(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d58f7-79c5-4c41-a627-fb6064b3ed91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:30.972982Z",
     "iopub.status.busy": "2025-04-13T17:18:30.972749Z",
     "iopub.status.idle": "2025-04-13T17:18:30.980301Z",
     "shell.execute_reply": "2025-04-13T17:18:30.979417Z",
     "shell.execute_reply.started": "2025-04-13T17:18:30.972962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The train() function takes the model, data loader, optimizer, scheduler, and device as its trainees. \n",
    "The function puts the model into training mode and then runs through each batch of data from the data loader. \n",
    "For each batch, it clears the optimizerâ€™s gradients, gets the input IDs, attention masks, and labels, and feeds them to the model\n",
    "'''\n",
    "def train_classifier(model, dataloader, optimizer, criterion, device, epochs):\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        \n",
    "        for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(dataloader):\n",
    "                print(f\"  Batch {batch_idx+1}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acdaf5f1-27da-49ec-aad5-4c6d5f0919c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:30.981884Z",
     "iopub.status.busy": "2025-04-13T17:18:30.981596Z",
     "iopub.status.idle": "2025-04-13T17:18:30.998510Z",
     "shell.execute_reply": "2025-04-13T17:18:30.997740Z",
     "shell.execute_reply.started": "2025-04-13T17:18:30.981847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0d352aa-4b5f-4e7a-8289-17b73272e9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:18:31.067708Z",
     "iopub.status.busy": "2025-04-13T17:18:31.067430Z",
     "iopub.status.idle": "2025-04-13T17:22:00.892134Z",
     "shell.execute_reply": "2025-04-13T17:22:00.891169Z",
     "shell.execute_reply.started": "2025-04-13T17:18:31.067688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved weights found. Training from scratch.\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "  Batch 10/163 - Loss: 0.5576\n",
      "  Batch 20/163 - Loss: 0.3615\n",
      "  Batch 30/163 - Loss: 0.2858\n",
      "  Batch 40/163 - Loss: 0.2082\n",
      "  Batch 50/163 - Loss: 0.1521\n",
      "  Batch 60/163 - Loss: 0.1077\n",
      "  Batch 70/163 - Loss: 0.1974\n",
      "  Batch 80/163 - Loss: 0.0171\n",
      "  Batch 90/163 - Loss: 0.0315\n",
      "  Batch 100/163 - Loss: 0.0486\n",
      "  Batch 110/163 - Loss: 0.0218\n",
      "  Batch 120/163 - Loss: 0.0044\n",
      "  Batch 130/163 - Loss: 0.0168\n",
      "  Batch 140/163 - Loss: 0.0061\n",
      "  Batch 150/163 - Loss: 0.0863\n",
      "  Batch 160/163 - Loss: 0.0439\n",
      "  Batch 163/163 - Loss: 0.0664\n",
      "Epoch [1/3] Average Loss: 0.1294\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "  Batch 10/163 - Loss: 0.0667\n",
      "  Batch 20/163 - Loss: 0.0061\n",
      "  Batch 30/163 - Loss: 0.0453\n",
      "  Batch 40/163 - Loss: 0.0404\n",
      "  Batch 50/163 - Loss: 0.0439\n",
      "  Batch 60/163 - Loss: 0.0021\n",
      "  Batch 70/163 - Loss: 0.1007\n",
      "  Batch 80/163 - Loss: 0.0018\n",
      "  Batch 90/163 - Loss: 0.0465\n",
      "  Batch 100/163 - Loss: 0.0037\n",
      "  Batch 110/163 - Loss: 0.0014\n",
      "  Batch 120/163 - Loss: 0.0025\n",
      "  Batch 130/163 - Loss: 0.0521\n",
      "  Batch 140/163 - Loss: 0.0012\n",
      "  Batch 150/163 - Loss: 0.0688\n",
      "  Batch 160/163 - Loss: 0.0537\n",
      "  Batch 163/163 - Loss: 0.0906\n",
      "Epoch [2/3] Average Loss: 0.0406\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "  Batch 10/163 - Loss: 0.0758\n",
      "  Batch 20/163 - Loss: 0.0476\n",
      "  Batch 30/163 - Loss: 0.0176\n",
      "  Batch 40/163 - Loss: 0.0587\n",
      "  Batch 50/163 - Loss: 0.0259\n",
      "  Batch 60/163 - Loss: 0.0434\n",
      "  Batch 70/163 - Loss: 0.0444\n",
      "  Batch 80/163 - Loss: 0.0016\n",
      "  Batch 90/163 - Loss: 0.0376\n",
      "  Batch 100/163 - Loss: 0.0456\n",
      "  Batch 110/163 - Loss: 0.0011\n",
      "  Batch 120/163 - Loss: 0.0248\n",
      "  Batch 130/163 - Loss: 0.0260\n",
      "  Batch 140/163 - Loss: 0.0917\n",
      "  Batch 150/163 - Loss: 0.0036\n",
      "  Batch 160/163 - Loss: 0.0618\n",
      "  Batch 163/163 - Loss: 0.0023\n",
      "Epoch [3/3] Average Loss: 0.0356\n",
      "Classifier saved to caption_classifier.pt\n"
     ]
    }
   ],
   "source": [
    "# Set device and initialize the tokenizer for BERT\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create dataset objects for train, validation, and test splits\n",
    "train_dataset = CaptionClassifierDataset(train_data, tokenizer, max_length=128)\n",
    "val_dataset = CaptionClassifierDataset(val_data, tokenizer, max_length=128)\n",
    "test_dataset = CaptionClassifierDataset(test_data, tokenizer, max_length=128)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the classifier, optimizer, and loss function\n",
    "model = CaptionClassifier(bert_model_name,2).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "weights_path = \"/kaggle/working/caption_classifier.pt\"\n",
    "if os.path.exists(weights_path):\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    print(f\"Loaded model weights from {weights_path}\")\n",
    "else:\n",
    "    print(\"No saved weights found. Training from scratch.\")\n",
    "\n",
    "\n",
    "# Train the classifier for a few epochs (adjust epochs as needed)\n",
    "epochs = 3\n",
    "train_classifier(model, train_loader, optimizer, criterion, device, epochs)\n",
    "model_save_path = \"caption_classifier.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Classifier saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2127dd25-ada0-4d6d-bb25-3fea6e9238dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T17:22:00.893683Z",
     "iopub.status.busy": "2025-04-13T17:22:00.893378Z",
     "iopub.status.idle": "2025-04-13T17:22:13.359210Z",
     "shell.execute_reply": "2025-04-13T17:22:13.358386Z",
     "shell.execute_reply.started": "2025-04-13T17:22:00.893660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9798\n",
      "Test Accuracy: 0.9838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       743\n",
      "           1       0.99      0.98      0.98       743\n",
      "\n",
      "    accuracy                           0.98      1486\n",
      "   macro avg       0.98      0.98      0.98      1486\n",
      "weighted avg       0.98      0.98      0.98      1486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation and test sets\n",
    "val_accuracy, val_report = evaluate_classifier(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_accuracy, test_report = evaluate_classifier(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(test_report)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7070169,
     "sourceId": 11305333,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
